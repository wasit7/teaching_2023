{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5151afed",
   "metadata": {},
   "source": [
    "# Retrieval-augmented generation (RAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14b744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.0.331 langchainhub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de1ee91-36f5-4cad-b901-4ca58f822785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de0b8ff-456b-486a-bb46-d9758b92a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89a56f1-5be5-4af4-baf7-af1e30c4f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79cb5724-a412-4876-a1f0-8ae93fc05838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f6b562-5e7a-4375-96d8-6503962b3f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set env var OPENAI_API_KEY or load from a .env file\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e646a64-0c58-40e8-a399-0d54aa60820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbaaf367-ae97-458b-a5c3-3f7b6126c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fb739a6-d6e3-46ab-983b-58e5a1f905af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "# vectorstore.save_local('vector.db')\n",
    "# vectorstore=FAISS.load_local('vector.db', embeddings=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc5fafd3-9ed3-4c24-863b-3ad89b7c3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import SVMRetriever\n",
    "# question=\"What are key facts from the articles?\"\n",
    "# svm_retriever = SVMRetriever.from_documents(splits, OpenAIEmbeddings())\n",
    "# docs_svm = svm_retriever.get_relevant_documents(question)\n",
    "# len(docs_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada5dee7-9653-4a5b-8eed-0376ea566507",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use five sentences maximum and keep the answer as concise as possible.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "rag_prompt_custom = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} | rag_prompt_custom | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e230626c-6173-400d-87a8-158db6f78454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Building agents with LLM (large language model) as its core controller is a cool concept.\n",
      "2. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer, and BabyAGI, serve as inspiring examples.\n",
      "3. LLM has the potential to be a powerful general problem solver.\n",
      "4. In a LLM-powered autonomous agent system, LLM functions as the agent's brain, complemented by several key components.\n",
      "5. The agent system overview includes components such as internet access for searches and information gathering, long-term memory management, GPT-3.5 powered agents for delegation of simple tasks, and file output.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"What are key facts from the articles?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "690aee77-8b55-4945-9510-691f1853687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The publish date of the article is June 2023.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"What is the publish date of the article?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91b85b87-af98-414b-9f74-572644dae005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import GPT4All\n",
    "# local_path = (\n",
    "#     \"./models/ggml-gpt4all-l13b-snoozy.bin\"  # replace with your desired local file path\n",
    "# )\n",
    "\n",
    "# # Callbacks support token-wise streaming\n",
    "# callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# # Verbose is required to pass to the callback manager\n",
    "# llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a86f0d-9a77-4c61-9209-96b3c1f37136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e05fa-34d2-40c1-b5a0-92110ab608c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
