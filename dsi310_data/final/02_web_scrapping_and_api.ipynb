{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f1c81b-26a5-4a4d-bc9f-65aa30c3344a",
   "metadata": {},
   "source": [
    "# Quizzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec90c40-79d4-4655-930f-73f4161fa585",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "File management and web scraping in Python, using real-world websites and scenarios. Each question will be designed to test and enhance different aspects of these skills. \n",
    "\n",
    "* Quiz 1: Basic File Reading and Writing\n",
    "* Quiz 2: Web Scraping Basic HTML Data\n",
    "* Quiz 3: Web Scraping with Pagination\n",
    "* Quiz 4: Advanced File Operations\n",
    "* Quiz 5: Web Scraping Dynamic Content\n",
    "* Quiz 6: Extracting and Analyzing Data from API\n",
    "* Quiz 7: Scraping and Processing E-commerce Product Data\n",
    "* Quiz 8: Automated Data Cleaning from a Text File\n",
    "* Quiz 9: Parsing and Summarizing Data from a News API\n",
    "* Quiz 10: Web Scraping with JavaScript-Rendered Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109a5dc1-7d34-465d-8c9a-4c3e5d75b237",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 1: Basic File Reading and Writing\n",
    "**Task**: Write a Python script to read a CSV file containing movie data from [IMDb](https://www.imdb.com/interfaces/), then convert and save this data into a JSON file. The script should be able to handle basic data cleaning like trimming whitespace from strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c68a4dd-2840-4435-b7d2-d3bcb949c8dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10345990 entries, 0 to 10345989\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Dtype \n",
      "---  ------          ----- \n",
      " 0   Unnamed: 0      int64 \n",
      " 1   tconst          object\n",
      " 2   titleType       object\n",
      " 3   primaryTitle    object\n",
      " 4   originalTitle   object\n",
      " 5   isAdult         object\n",
      " 6   startYear       object\n",
      " 7   endYear         object\n",
      " 8   runtimeMinutes  object\n",
      " 9   genres          object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 789.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tconst</th>\n",
       "      <th>titleType</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>isAdult</th>\n",
       "      <th>startYear</th>\n",
       "      <th>endYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tt0000001</td>\n",
       "      <td>short</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>0</td>\n",
       "      <td>1894</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Documentary,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0000002</td>\n",
       "      <td>short</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>5</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tt0000003</td>\n",
       "      <td>short</td>\n",
       "      <td>Pauvre Pierrot</td>\n",
       "      <td>Pauvre Pierrot</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>4</td>\n",
       "      <td>Animation,Comedy,Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tt0000004</td>\n",
       "      <td>short</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>0</td>\n",
       "      <td>1892</td>\n",
       "      <td>\\N</td>\n",
       "      <td>12</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tt0000005</td>\n",
       "      <td>short</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>0</td>\n",
       "      <td>1893</td>\n",
       "      <td>\\N</td>\n",
       "      <td>1</td>\n",
       "      <td>Comedy,Short</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     tconst titleType            primaryTitle  \\\n",
       "0           0  tt0000001     short              Carmencita   \n",
       "1           1  tt0000002     short  Le clown et ses chiens   \n",
       "2           2  tt0000003     short          Pauvre Pierrot   \n",
       "3           3  tt0000004     short             Un bon bock   \n",
       "4           4  tt0000005     short        Blacksmith Scene   \n",
       "\n",
       "            originalTitle isAdult startYear endYear runtimeMinutes  \\\n",
       "0              Carmencita       0      1894      \\N              1   \n",
       "1  Le clown et ses chiens       0      1892      \\N              5   \n",
       "2          Pauvre Pierrot       0      1892      \\N              4   \n",
       "3             Un bon bock       0      1892      \\N             12   \n",
       "4        Blacksmith Scene       0      1893      \\N              1   \n",
       "\n",
       "                     genres  \n",
       "0         Documentary,Short  \n",
       "1           Animation,Short  \n",
       "2  Animation,Comedy,Romance  \n",
       "3           Animation,Short  \n",
       "4              Comedy,Short  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quiz_01\n",
    "#data source: https://developer.imdb.com/non-commercial-datasets/\n",
    "import pandas as pd\n",
    "# file_path = 'https://datasets.imdbws.com/title.basics.tsv.gz'\n",
    "file_path = './dataset/title.basics.tsv.gz'\n",
    "df=pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab0ab1-94c2-4da6-a5b5-609b8f45f0fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 2: Web Scraping Basic HTML Data\n",
    "**Task**: Write a Python script using `BeautifulSoup` to scrape the current top news headlines from [BBC News](https://www.bbc.com/news). Extract the headline text and the corresponding URLs, and save them in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5a932ef-48d0-4e2e-8eab-443226f3e0d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43 entries, 0 to 42\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   headline  43 non-null     object\n",
      " 1   url       43 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 816.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Three killed in Jerusalem shooting hours after...</td>\n",
       "      <td>https://www.bbc.com/news/live/world-middle-eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thai hostages return to overjoyed families</td>\n",
       "      <td>https://www.bbc.com/news/world-asia-67563914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ex-US Secretary of State Henry Kissinger dies ...</td>\n",
       "      <td>https://www.bbc.com/news/world-us-canada-67574495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nepal registers first same-sex marriage</td>\n",
       "      <td>https://www.bbc.com/news/world-asia-67574710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>North Korea says it's got eyes on the White Ho...</td>\n",
       "      <td>https://www.bbc.com/news/world-asia-67563543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Three killed in Jerusalem shooting hours after...   \n",
       "1         Thai hostages return to overjoyed families   \n",
       "2  Ex-US Secretary of State Henry Kissinger dies ...   \n",
       "3            Nepal registers first same-sex marriage   \n",
       "4  North Korea says it's got eyes on the White Ho...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.bbc.com/news/live/world-middle-eas...  \n",
       "1       https://www.bbc.com/news/world-asia-67563914  \n",
       "2  https://www.bbc.com/news/world-us-canada-67574495  \n",
       "3       https://www.bbc.com/news/world-asia-67574710  \n",
       "4       https://www.bbc.com/news/world-asia-67563543  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Fetching the webpage\n",
    "response = requests.get(\"https://www.bbc.com/news\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extracting headlines and URLs\n",
    "articles = soup.find_all('h3')\n",
    "data = [{'headline': article.get_text(strip=True), 'url': 'https://www.bbc.com' + article.find_parent('a')['href']} for article in articles if article.find_parent('a')]\n",
    "\n",
    "# Saving to CSV\n",
    "pd.DataFrame(data).to_csv('./dataset/bbc_news_headlines.csv', index=False)\n",
    "df = pd.read_csv('./dataset/bbc_news_headlines.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0409cd4-f1dc-4e82-a687-d265e5b6e331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.bbc.com/news/live/world-middle-east-67562488'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244f822-7ee7-4809-ad41-9b32e6719744",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 3: Web Scraping with Pagination\n",
    "**Task**: Create a Python script to scrape job listings from the first three pages of [Indeed](https://www.indeed.com) for a specific job title and location. The script should extract the job title, company name, location, and summary of each listing and save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e31c9-e5aa-4bf6-a208-df48431b66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "import time\n",
    "\n",
    "async def scrape_indeed_jobs(job_title, location):\n",
    "    job_listings = []\n",
    "    pw = await async_playwright().start()\n",
    "    browser = await pw.chromium.launch(headless = False)\n",
    "    context = await browser.new_context()\n",
    "    page = await context.new_page()\n",
    "    for start in range(0, 30, 15):  # 0, 10, 20 for the first three pages\n",
    "        time.sleep(5)\n",
    "        params = {\n",
    "            'q': job_title,\n",
    "            'l': location,\n",
    "            'start': start\n",
    "        }\n",
    "        url=f'https://th.indeed.com/jobs?{urlencode(params)}'\n",
    "        _ = await page.goto(url)\n",
    "        \n",
    "        await page.wait_for_selector('div#mosaic-jobResults')\n",
    "        selector = await page.query_selector('body')\n",
    "        html = await selector.inner_html()\n",
    "    \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        for job_card in soup.find_all('div', class_='cardOutline'):\n",
    "            title = job_card.find('h2', class_='jobTitle').get_text(strip=True)\n",
    "            company = job_card.find('span', {\"data-testid\" : \"company-name\"}).get_text(strip=True)\n",
    "            location = job_card.find('div', {\"data-testid\" : \"text-location\"}).get_text(strip=True) if job_card.find('div', {\"data-testid\" : \"text-location\"}) else 'N/A'\n",
    "            # summary = job_card.find('div', class_='summary').get_text(strip=True)\n",
    "            job_listings.append({'Job Title': title, 'Company': company, 'Location': location})\n",
    "    \n",
    "    await browser.close()\n",
    "    await pw.stop()\n",
    "    # print(job_listings)\n",
    "    return job_listings\n",
    "\n",
    "jobs = await scrape_indeed_jobs('software engineer', 'Pathum Thani')\n",
    "pd.DataFrame(jobs).to_csv('./dataset/indeed_job_listings.csv', index=False)\n",
    "df=pd.read_csv('./dataset/indeed_job_listings.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d89efb-b511-4504-acce-4a3ccc1cd6a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 4: Advanced File Operations\n",
    "**Task**: Write a Python script to scan a directory containing log files (text files). The script should aggregate error messages from all files, count their occurrences, and output a summary in a new text file. Assume a specific pattern in the log files denotes errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5560776-9f79-4549-8e4a-e9846c53b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data source https://github.com/logpai/loghub/blob/master/Linux/Linux_2k.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b4ad7db-9dab-4c9e-b676-3a9f32eda40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1546 entries, 0 to 1545\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   datetime  1546 non-null   object\n",
      " 1   severity  1546 non-null   object\n",
      " 2   message   1546 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 36.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>severity</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jun 14 15:16:01</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>combo sshd(pam_unix)[19939]: authentication fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jun 14 15:16:02</td>\n",
       "      <td>WARNING</td>\n",
       "      <td>combo sshd(pam_unix)[19937]: check pass; user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jun 14 15:16:02</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>combo sshd(pam_unix)[19937]: authentication fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jun 15 02:04:59</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>combo sshd(pam_unix)[20882]: authentication fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jun 15 02:04:59</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>combo sshd(pam_unix)[20884]: authentication fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          datetime severity                                            message\n",
       "0  Jun 14 15:16:01    ERROR  combo sshd(pam_unix)[19939]: authentication fa...\n",
       "1  Jun 14 15:16:02  WARNING  combo sshd(pam_unix)[19937]: check pass; user ...\n",
       "2  Jun 14 15:16:02    ERROR  combo sshd(pam_unix)[19937]: authentication fa...\n",
       "3  Jun 15 02:04:59    ERROR  combo sshd(pam_unix)[20882]: authentication fa...\n",
       "4  Jun 15 02:04:59    ERROR  combo sshd(pam_unix)[20884]: authentication fa..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_linux_log(file_path):\n",
    "    log_data = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Regex pattern to extract datetime and the entire message\n",
    "            match = re.match(r'(\\w{3} \\d{1,2} \\d{2}:\\d{2}:\\d{2}) (.*)', line)\n",
    "            if match:\n",
    "                datetime, message = match.groups()\n",
    "                severity = \"ERROR\" if \"failure\" in message else \"WARNING\"  # Assuming 'failure' indicates an error\n",
    "                log_data.append({'datetime': datetime, 'severity': severity, 'message': message})\n",
    "\n",
    "    return pd.DataFrame(log_data)\n",
    "\n",
    "# Example usage\n",
    "log_file_path = './dataset/Linux_2k.log'  # Replace with the actual path\n",
    "df = parse_linux_log(log_file_path)\n",
    "df.to_csv('./dataset/structured_log_data.csv', index=False)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0bf64-a789-4394-b7ec-59e41b890751",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 5: Web Scraping Dynamic Content\n",
    "**Task**: Use Python with Selenium to scrape the latest tech news articles from [TechCrunch](https://techcrunch.com/). The script should navigate the site, handle dynamic content loading, and extract the article titles, authors, and publication dates, saving them in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a45b7ef-5d4b-434e-9029-8bb5c5545fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Title             20 non-null     object\n",
      " 1   Author            20 non-null     object\n",
      " 2   Publication Date  20 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 608.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publication Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It’s official: Evernote will restrict free use...</td>\n",
       "      <td>Ivan Mehta</td>\n",
       "      <td>1:39 PM GMT+7•November 30, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robinhood brings its stock-trading platform to...</td>\n",
       "      <td>Paul Sawers</td>\n",
       "      <td>1:00 PM GMT+7•November 30, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>European consumer groups band together to figh...</td>\n",
       "      <td>Natasha Lomas</td>\n",
       "      <td>12:00 PM GMT+7•November 30, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple and Google avoid naming ChatGPT as their...</td>\n",
       "      <td>Sarah Perez</td>\n",
       "      <td>11:00 AM GMT+7•November 30, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A timeline of Sam Altman’s firing from OpenAI ...</td>\n",
       "      <td>Kyle Wiggers</td>\n",
       "      <td>9:16 AM GMT+7•November 30, 2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title         Author  \\\n",
       "0  It’s official: Evernote will restrict free use...     Ivan Mehta   \n",
       "1  Robinhood brings its stock-trading platform to...    Paul Sawers   \n",
       "2  European consumer groups band together to figh...  Natasha Lomas   \n",
       "3  Apple and Google avoid naming ChatGPT as their...    Sarah Perez   \n",
       "4  A timeline of Sam Altman’s firing from OpenAI ...   Kyle Wiggers   \n",
       "\n",
       "                   Publication Date  \n",
       "0   1:39 PM GMT+7•November 30, 2023  \n",
       "1   1:00 PM GMT+7•November 30, 2023  \n",
       "2  12:00 PM GMT+7•November 30, 2023  \n",
       "3  11:00 AM GMT+7•November 30, 2023  \n",
       "4   9:16 AM GMT+7•November 30, 2023  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "pw = await async_playwright().start()\n",
    "browser = await pw.chromium.launch(headless = False)\n",
    "page = await browser.new_page()\n",
    "url=f'https://techcrunch.com/'\n",
    "_ = await page.goto(url)\n",
    "await page.wait_for_selector('div.content')\n",
    "\n",
    "selector = await page.query_selector('body')\n",
    "html = await selector.inner_html()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# print(soup)\n",
    "\n",
    "articles =  soup.find_all('article', class_=\"post-block\")\n",
    "\n",
    "data = []\n",
    "for article in articles:\n",
    "    # if not article!=article: break\n",
    "    title =  article.find('h2').get_text(strip=True) \n",
    "    author =  article.find('span', class_ = 'river-byline__authors').get_text(strip=True)\n",
    "    datetime =  article.find('time', class_ = 'river-byline__full-date-time').get_text(strip=True)\n",
    "    data.append({'Title': title, 'Author': author, 'Publication Date': datetime})\n",
    "\n",
    "await browser.close()\n",
    "await pw.stop()\n",
    "df=pd.DataFrame(data)\n",
    "df.to_csv('./dataset/techcrunch.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df86608e-9f99-45db-9e98-4b9c7de5ccb9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 6: Extracting and Analyzing Data from API\n",
    "**Task**: Write a Python script to fetch weather data from the [OpenWeatherMap API](https://openweathermap.org/api). Extract temperature, humidity, and weather conditions for a specified city, and write this data to a JSON file. Include error handling for invalid city names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2715298b-80c7-4430-8610-f1e87fe7dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "\n",
    "async def fetch_json(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def fetch_news_content(session, url, is_pdf):\n",
    "    if is_pdf:\n",
    "        return \"\"\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "base_url = 'https://www.ditp.go.th'\n",
    "posts_url = f'{base_url}/wp-json/ditp/v1/posts?offset=0&limit=20'\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    posts_data = await fetch_json(session, posts_url)\n",
    "    tasks = []\n",
    "    for post in posts_data['data']:\n",
    "        news_url = f'{base_url}/post/{post[\"ContentID\"]}'\n",
    "        tasks.append( fetch_news_content(session, news_url, post[\"PDF\"]))\n",
    "    contents = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3d342e3-9f10-46b3-9235-7874a4d89801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   title         20 non-null     object\n",
      " 1   publish_date  20 non-null     object\n",
      " 2   source_url    20 non-null     object\n",
      " 3   is_pdf        10 non-null     object\n",
      " 4   content       20 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 928.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>source_url</th>\n",
       "      <th>is_pdf</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>พฤติกรรมผู้บริโภคของ GEN Z และการเปลี่ยนแปลงขอ...</td>\n",
       "      <td>2566-11-30 15:10:23</td>\n",
       "      <td>https://www.ditp.go.th/post/155171</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"th\" prefix=\"og: h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ประกาศกรมส่งเสริมการค้าระหว่างประเทศ เรื่อง ปร...</td>\n",
       "      <td>2566-11-30 14:56:06</td>\n",
       "      <td>https://www.ditp.go.th/post/155208</td>\n",
       "      <td>[https://www.ditp.go.th/files/155210]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ประกาศกรมส่งเสริมการค้าระหว่างประเทศ เรื่อง เผ...</td>\n",
       "      <td>2566-11-30 14:52:09</td>\n",
       "      <td>https://www.ditp.go.th/post/155205</td>\n",
       "      <td>[https://www.ditp.go.th/files/155206]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fact Sheet China October 2023</td>\n",
       "      <td>2566-11-30 14:35:59</td>\n",
       "      <td>https://www.ditp.go.th/post/155192</td>\n",
       "      <td>[https://www.ditp.go.th/files/155196]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>เซ็นแล้ว 5,000 ตัน! กล้วยอีสานพร้อมผงาดในตลาดญ...</td>\n",
       "      <td>2566-11-30 13:00:08</td>\n",
       "      <td>https://www.ditp.go.th/post/155165</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;html&gt;&lt;body&gt;&lt;h1&gt;504 Gateway Time-out&lt;/h1&gt;\\nThe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title         publish_date  \\\n",
       "0  พฤติกรรมผู้บริโภคของ GEN Z และการเปลี่ยนแปลงขอ...  2566-11-30 15:10:23   \n",
       "1  ประกาศกรมส่งเสริมการค้าระหว่างประเทศ เรื่อง ปร...  2566-11-30 14:56:06   \n",
       "2  ประกาศกรมส่งเสริมการค้าระหว่างประเทศ เรื่อง เผ...  2566-11-30 14:52:09   \n",
       "3                      Fact Sheet China October 2023  2566-11-30 14:35:59   \n",
       "4  เซ็นแล้ว 5,000 ตัน! กล้วยอีสานพร้อมผงาดในตลาดญ...  2566-11-30 13:00:08   \n",
       "\n",
       "                           source_url                                 is_pdf  \\\n",
       "0  https://www.ditp.go.th/post/155171                                   None   \n",
       "1  https://www.ditp.go.th/post/155208  [https://www.ditp.go.th/files/155210]   \n",
       "2  https://www.ditp.go.th/post/155205  [https://www.ditp.go.th/files/155206]   \n",
       "3  https://www.ditp.go.th/post/155192  [https://www.ditp.go.th/files/155196]   \n",
       "4  https://www.ditp.go.th/post/155165                                   None   \n",
       "\n",
       "                                             content  \n",
       "0  <!DOCTYPE html>\\n<html lang=\"th\" prefix=\"og: h...  \n",
       "1                                                     \n",
       "2                                                     \n",
       "3                                                     \n",
       "4  <html><body><h1>504 Gateway Time-out</h1>\\nThe...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents\n",
    "data = [{\n",
    "        'title': post['Title'],\n",
    "        'publish_date': post['PublishDate'],\n",
    "        'source_url': f'{base_url}/post/{post[\"ContentID\"]}',\n",
    "        'is_pdf': post['PDF'],\n",
    "        'content': content\n",
    "        } for post, content in zip(posts_data['data'], contents)]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4769154-0325-437c-8391-14295a1fd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f457d65d-468e-4a62-ad9f-9ac6c0e83c2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 7: Scraping and Processing E-commerce Product Data\n",
    "**Task**: Create a Python script to scrape product details from an e-commerce site like [Amazon](https://www.amazon.com). Focus on a specific category (e.g., books, electronics). Extract product names, prices, and ratings, and save them in a pandas DataFrame for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f86d6e-a91a-4963-9e4c-4efe6473b9b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 8: Automated Data Cleaning from a Text File\n",
    "**Task**: Write a Python script to read a text file from [Project Gutenberg](https://www.gutenberg.org/). The script should remove all the headers and footers added by Project Gutenberg, count the frequency of each word in the text, and output the top 10 most frequent words to a new file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c3bdb-ce60-4954-a5dd-d1456c6ee0e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 9: Parsing and Summarizing Data from a News API\n",
    "**Task**: Use the [News API](https://newsapi.org/) to fetch recent news articles on a specific topic (e.g., \"climate change\"). Write a Python script to parse this data, extracting the article title, source, and publication date, and then summarize this data in a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe4d150-92c6-46a1-bca4-75f601fd1f83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quiz 10: Web Scraping with JavaScript-Rendered Content\n",
    "**Task**: Write a Python script using Selenium to scrape movie ratings and reviews from a site like [Rotten Tomatoes](https://www.rottentomatoes.com/). The script should navigate through a list of movies, handle the dynamically loaded content, and extract the movie title, rating, and a sample of user reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
