{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d065b-28e6-48fa-b189-0baa0a9b56a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fsspec gcsfs pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5629382-f2c7-4132-a30b-199b92054048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fsspec\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def tree(fs, path, prefix=\"\"):\n",
    "    \"\"\"Recursively list the contents of a directory in a tree-like format.\"\"\"\n",
    "    # print(path)\n",
    "    items = fs.ls(path, detail=True)\n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == (len(items) - 1)  # Check if this is the last item\n",
    "        if item['type'] == 'directory':\n",
    "            # Print the directory name\n",
    "            print(f\"{prefix}{'└── ' if is_last else '├── '}{item['name'].split('/')[-1]}\")\n",
    "            # Recursively list this directory's contents\n",
    "            new_prefix = prefix + ('    ' if is_last else '│   ')\n",
    "            tree(fs, item['name'], new_prefix)\n",
    "        else:\n",
    "            # Print the file name\n",
    "            print(f\"{prefix}{'└── ' if is_last else '├── '}{item['name'].split('/')[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf25db13-33b9-462e-9249-c1e58de6e1b7",
   "metadata": {},
   "source": [
    "### Local File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b75e25-7bca-4ef8-8bd2-ab5ae8047929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = fsspec.filesystem('file',auto_mkdir=True) #fsspec.filesystem(catalog_path, auto_mkdir=False)\n",
    "\n",
    "# catalog_path='./catalog/'\n",
    "# with fs.open(catalog_path+'readme.md','wb') as f:\n",
    "#     f.write(b'# Hello')\n",
    "    \n",
    "# tree(fs,catalog_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dbc12c-4238-4173-9eb8-6104e3866d17",
   "metadata": {},
   "source": [
    "### Google Cloud Storage(GCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c083b78d-3b5d-4d0d-b449-e0d211e4a1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── group01\n",
      "│   └── \n",
      "├── group02\n",
      "│   └── \n",
      "├── group03\n",
      "│   └── \n",
      "├── group04\n",
      "│   └── \n",
      "├── group05\n",
      "│   └── \n",
      "├── group06\n",
      "│   └── \n",
      "├── group07\n",
      "│   └── \n",
      "├── group08\n",
      "│   └── \n",
      "├── group09\n",
      "│   └── \n",
      "├── group10\n",
      "│   └── \n",
      "├── group11\n",
      "│   └── \n",
      "├── group12\n",
      "│   └── \n",
      "└── readme.md\n"
     ]
    }
   ],
   "source": [
    "gcs_token_path = os.path.join('../_env/teacher-dsi310-2023.json')\n",
    "catalog_path = 'gcs://dsi310_bucket/'\n",
    "fs=fsspec.filesystem('gcs', token=gcs_token_path)\n",
    "with fs.open(catalog_path+'readme.md','wb') as f:\n",
    "    f.write(b'# Hello')\n",
    "tree(fs,catalog_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b982e19-e678-4535-a861-04c3729fd84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date: timestamp[ns]\n",
       "product_id: int64\n",
       "quantity: int64\n",
       "price: double\n",
       "-- schema metadata --\n",
       "metadata: '{'source': 'Sales System', 'creation_date': '2023-12-05T21:12:' + 11\n",
       "dictionary: '{'date': 'Transaction date', 'product_id': 'Product identifi' + 56"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range(start='2023-01-01', periods=5, freq='D'),\n",
    "    'product_id': range(1, 6),\n",
    "    'quantity': [5, 3, 6, 2, 7],\n",
    "    'price': [20.5, 10.0, 15.5, 25.0, 30.0]\n",
    "})\n",
    "\n",
    "# Metadata and Data Dictionary\n",
    "metadata = {'source': 'Sales System', 'creation_date': datetime.now().isoformat()}\n",
    "data_dictionary = {\n",
    "    'date': 'Transaction date',\n",
    "    'product_id': 'Product identifier',\n",
    "    'quantity': 'Quantity sold',\n",
    "    'price': 'Sale price'\n",
    "}\n",
    "\n",
    "# Convert DataFrame to PyArrow Table with metadata\n",
    "table = pa.Table.from_pandas(df)\n",
    "table = table.replace_schema_metadata({'metadata': str(metadata), 'dictionary': str(data_dictionary)})\n",
    "\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4197f540-4a27-41b8-bc62-5939fbee81cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'source': 'Sales System', 'creation_date': '2023-12-05T21:12:52.450270'}\n",
      "Data Dictionary: {'date': 'Transaction date', 'product_id': 'Product identifier', 'quantity': 'Quantity sold', 'price': 'Sale price'}\n"
     ]
    }
   ],
   "source": [
    "# Extracting metadata and data dictionary\n",
    "metadata = eval(table.schema.metadata[b'metadata']) if b'metadata' in table.schema.metadata else None\n",
    "data_dictionary = eval(table.schema.metadata[b'dictionary']) if b'dictionary' in table.schema.metadata else None\n",
    "\n",
    "# Display the DataFrame, Metadata, and Data Dictionary\n",
    "print(\"Metadata:\", metadata)\n",
    "print(\"Data Dictionary:\", data_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30225f20-9f4a-4219-b49b-1e93b173a651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── group01\n",
      "│   └── \n",
      "├── group02\n",
      "│   └── \n",
      "├── group03\n",
      "│   └── \n",
      "├── group04\n",
      "│   └── \n",
      "├── group05\n",
      "│   └── \n",
      "├── group06\n",
      "│   └── \n",
      "├── group07\n",
      "│   └── \n",
      "├── group08\n",
      "│   └── \n",
      "├── group09\n",
      "│   └── \n",
      "├── group10\n",
      "│   └── \n",
      "├── group11\n",
      "│   └── \n",
      "├── group12\n",
      "│   └── \n",
      "├── readme.md\n",
      "└── sale.parquet\n"
     ]
    }
   ],
   "source": [
    "# Write to GCS\n",
    "# gcs_path = 'gcs://dsi310_bucket/sales_data.parquet'  # Replace with your bucket path\n",
    "with fs.open(path=catalog_path+'sale.parquet',mode='wb') as f:  # Replace with your GCS token\n",
    "    pq.write_table(table, f)\n",
    "\n",
    "tree(fs,catalog_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcbd5612-55ff-4a64-80db-52d922874e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>product_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>20.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>15.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  product_id  quantity  price\n",
       "0  2023-01-01           1         5   20.5\n",
       "1  2023-01-02           2         3   10.0\n",
       "2  2023-01-03           3         6   15.5\n",
       "3  2023-01-04           4         2   25.0\n",
       "4  2023-01-05           5         7   30.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fsspec\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "\n",
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range(start='2023-01-01', periods=5, freq='D'),\n",
    "    'product_id': range(1, 6),\n",
    "    'quantity': [5, 3, 6, 2, 7],\n",
    "    'price': [20.5, 10.0, 15.5, 25.0, 30.0]\n",
    "})\n",
    "df['date'] = df['date'].dt.date\n",
    "# Metadata and Data Dictionary\n",
    "metadata = {'source': 'Sales System', 'creation_date': datetime.now().isoformat()}\n",
    "data_dictionary = {\n",
    "    'date': 'Transaction date',\n",
    "    'product_id': 'Product identifier',\n",
    "    'quantity': 'Quantity sold',\n",
    "    'price': 'Sale price'\n",
    "}\n",
    "\n",
    "# Convert DataFrame to PyArrow Table with metadata\n",
    "table = pa.Table.from_pandas(df)\n",
    "table = table.replace_schema_metadata({'metadata': str(metadata), 'dictionary': str(data_dictionary)})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f511c512-00c1-41de-939a-032e6b20eec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcs://dsi310_bucket/sale'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define GCS path and write to GCS\n",
    "\n",
    "# No need to open a file with fsspec, use the path directly\n",
    "dataset_name ='sale'\n",
    "path = catalog_path+dataset_name\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb09502-352d-4abb-b4d5-73549b08d475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── group01\n",
      "│   └── \n",
      "├── group02\n",
      "│   └── \n",
      "├── group03\n",
      "│   └── \n",
      "├── group04\n",
      "│   └── \n",
      "├── group05\n",
      "│   └── \n",
      "├── group06\n",
      "│   └── \n",
      "├── group07\n",
      "│   └── \n",
      "├── group08\n",
      "│   └── \n",
      "├── group09\n",
      "│   └── \n",
      "├── group10\n",
      "│   └── \n",
      "├── group11\n",
      "│   └── \n",
      "├── group12\n",
      "│   └── \n",
      "├── sale\n",
      "│   ├── date=2023-01-01\n",
      "│   │   └── 5fd9f34bf82e4ad5ad8c5ca695be9705-0.parquet\n",
      "│   ├── date=2023-01-02\n",
      "│   │   └── 5fd9f34bf82e4ad5ad8c5ca695be9705-0.parquet\n",
      "│   ├── date=2023-01-03\n",
      "│   │   └── 5fd9f34bf82e4ad5ad8c5ca695be9705-0.parquet\n",
      "│   ├── date=2023-01-04\n",
      "│   │   └── 5fd9f34bf82e4ad5ad8c5ca695be9705-0.parquet\n",
      "│   └── date=2023-01-05\n",
      "│       └── 5fd9f34bf82e4ad5ad8c5ca695be9705-0.parquet\n",
      "├── readme.md\n",
      "└── sale.parquet\n"
     ]
    }
   ],
   "source": [
    "pq.write_to_dataset(table, root_path=path, partition_cols=['date'], filesystem=fs,)\n",
    "\n",
    "tree(fs,catalog_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4482768d-78e4-4b31-bbe6-f6066d6bbdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcs://dsi310_bucket/sale'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog_path+'sale'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9448e1ad-488d-43dc-83f9-1125bb642796",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "GetFileInfo() yielded path 'dsi310_bucket/sale/date=2023-01-05/5fd9f34bf82e4ad5ad8c5ca695be9705-0.parquet', which is outside base dir 'gcs://dsi310_bucket/sale'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mds\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# dataset = ds.dataset('/dsi310_bucket/sale/', format=\"parquet\", filesystem=fs)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatalog_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m table \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      8\u001b[0m table\u001b[38;5;241m.\u001b[39mschema\n",
      "File \u001b[0;32m~/miniconda3/envs/teaching_2023/lib/python3.10/site-packages/pyarrow/parquet/core.py:1793\u001b[0m, in \u001b[0;36mParquetDataset.__new__\u001b[0;34m(cls, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, read_dictionary, memory_map, buffer_size, partitioning, use_legacy_dataset, pre_buffer, coerce_int96_timestamp_unit, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   1790\u001b[0m         use_legacy_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_legacy_dataset:\n\u001b[0;32m-> 1793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ParquetDatasetV2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# unsupported keywords\u001b[39;49;00m\n\u001b[1;32m   1803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_nthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_nthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1810\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to get the legacy behaviour is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated as of pyarrow 11.0.0, and the legacy implementation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m extra_msg,\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/teaching_2023/lib/python3.10/site-packages/pyarrow/parquet/core.py:2507\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partitioning \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2504\u001b[0m     partitioning \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mHivePartitioning\u001b[38;5;241m.\u001b[39mdiscover(\n\u001b[1;32m   2505\u001b[0m         infer_dictionary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2507\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2508\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2509\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/teaching_2023/lib/python3.10/site-packages/pyarrow/dataset.py:782\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    771\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    772\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m    773\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    778\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mignore_prefixes\n\u001b[1;32m    779\u001b[0m )\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[0;32m--> 782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[0;32m~/miniconda3/envs/teaching_2023/lib/python3.10/site-packages/pyarrow/dataset.py:473\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    465\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m _ensure_single_source(source, filesystem)\n\u001b[1;32m    467\u001b[0m options \u001b[38;5;241m=\u001b[39m FileSystemFactoryOptions(\n\u001b[1;32m    468\u001b[0m     partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[1;32m    469\u001b[0m     partition_base_dir\u001b[38;5;241m=\u001b[39mpartition_base_dir,\n\u001b[1;32m    470\u001b[0m     exclude_invalid_files\u001b[38;5;241m=\u001b[39mexclude_invalid_files,\n\u001b[1;32m    471\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mselector_ignore_prefixes\n\u001b[1;32m    472\u001b[0m )\n\u001b[0;32m--> 473\u001b[0m factory \u001b[38;5;241m=\u001b[39m \u001b[43mFileSystemDatasetFactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths_or_selector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m factory\u001b[38;5;241m.\u001b[39mfinish(schema)\n",
      "File \u001b[0;32m~/miniconda3/envs/teaching_2023/lib/python3.10/site-packages/pyarrow/_dataset.pyx:3190\u001b[0m, in \u001b[0;36mpyarrow._dataset.FileSystemDatasetFactory.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/teaching_2023/lib/python3.10/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/teaching_2023/lib/python3.10/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: GetFileInfo() yielded path 'dsi310_bucket/sale/date=2023-01-05/5fd9f34bf82e4ad5ad8c5ca695be9705-0.parquet', which is outside base dir 'gcs://dsi310_bucket/sale'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyarrow.dataset as ds\n",
    "# dataset = ds.dataset('/dsi310_bucket/sale/', format=\"parquet\", filesystem=fs)\n",
    "\n",
    "\n",
    "dataset = pq.ParquetDataset(path_or_paths=catalog_path+'sale', filesystem=fs)\n",
    "table = dataset.read()\n",
    "\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930ae86-9569-40de-9647-4977f534c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting metadata and data dictionary\n",
    "metadata = eval(table.schema.metadata[b'metadata']) if b'metadata' in table.schema.metadata else None\n",
    "data_dictionary = eval(table.schema.metadata[b'dictionary']) if b'dictionary' in table.schema.metadata else None\n",
    "\n",
    "# Display the DataFrame, Metadata, and Data Dictionary\n",
    "print(\"Metadata:\", metadata)\n",
    "print(\"Data Dictionary:\", data_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f6e8ac-de27-4689-936b-b2b6e51cfc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdd6fa-59eb-4212-8958-a495c29a1b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569571c-7233-4fc5-9f4b-e5a36b36cb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
